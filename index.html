
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Kilian Fatras</title>
    <!-- Bootstrap core CSS -->
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
    <link href="main.css" rel="stylesheet">
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-48368675-1', 'auto');
    ga('send', 'pageview');
    </script>
  </head>
<body>
    <div class="container">
      <div class="row" style="padding:20px">
        <div class="hidden-xs col-sm-2 col-md-2" id="sidebar" role="navigation" style="margin-top:190px">
          <hr>
          <ul class="nav nav-pills nav-stacked">
            <li><a href="#research_interests" class="">Research interests</a></li>
            <li><a href="#Talks" class="">Talks</a></li>
            <li><a href="#papers" class=" active">Papers</a></li>
            <li><a href="#Workshops" class="">Workshops</a></li>
            <li><a href="#Lectures" class="">Lectures</a></li>
            <li><a href="#Projects" class="">Projects</a></li>
            <li><a href="#contact" class="">Contacts</a></li>
          </ul>
        </div>
        <div class="col-xs-13 col-sm-8 col-md-8">
          <div class="row">
            <div id="photo">
              <script type='text/javascript'>
              var proba_imgs = [ .1,.9]
              var images = [
              //"photos/id_circle2.png",
              //"photos/id_circle4.png",
              "images/kilian.png"
               //"images/wedgemount.jpg"
              ];
              var swapIndexA = Math.floor(Math.random() * images.length);
              var swapIndexB = Math.floor(Math.random() * images.length);
              var temp = images[swapIndexA];
              images[swapIndexA] = images[swapIndexB];
              images[swapIndexB] = temp;
              document.write("<img class=\"img-circle\" src='" + images.shift() + "' class=\"pull-left\" style=\"margin:20px 20px 20px 0; width:145px; height:150px; border-radius:100%\" onmouseover='this.src = this.src.replace(/(\\w+)(\\.\\w{3,4})$/, \"$1_over$2\");' onmouseout='this.src = this.src.replace(/_over\\./, \".\");' />");
              </script>
            </div>
            <h1>Dr. Kilian Fatras</h1>
            <p class="lead">PostDoctoral fellow at Mila laboratory and McGill University<br>
            </p>
          </div>
        </div>

        <div class="col-xs-13 col-sm-8 col-md-8">
          <div class="row">
            <hr>


              <p>Greetings and welcome to my personal website!</p>
              <p>I am a Postdoctoral fellow at Mila laboratory and McGill University in Montréal. I am 
                working under the supervision of Pr. <a href="https://www.adamoberman.net/">Adam Oberman</a> and Pr. <a href="http://mitliagkas.github.io/">Ioannis Mitliagkas</a>. 
                My current research focuses on generative models, protein generation and optimal transport. I am the co-creator and core maintainer of the 
                <a href="https://github.com/atong01/conditional-flow-matching">TorchCFM library</a> which open-sources our work on the novel Flow Matching generative models!</p>
              
              <p>Before my Postdoctoral fellowship, I completed my PhD under the supervision of Pr. <a href="http://people.irisa.fr/Nicolas.Courty/">Nicolas Courty</a> 
                and Pr. <a href="https://remi.flamary.com/">Rémi Flamary</a>  at IRISA-INRIA Panama and Obelix. My research focused on the interaction of optimal transport 
                and deep learning with applications to domain adaptation, noisy labels and generative modelling.
                The recording of my thesis defense can be found on <a href="https://www.youtube.com/watch?v=paqpidEnnHw">YouTube</a>,
                the slides <a href="pdf/slide_thesis_kf.pdf" class="">here</a> and the manuscript <a href="pdf/thesis_kf.pdf" class="">here</a>.</p>
              
              <p>I hold a degree in applied mathematics and machine learning from both Ecole Polytechnique and ENSTA ParisTech, 
                and was also an exchange student at UC Berkeley during the fall of 2018.</p>
              
              <p>If you are interested in learning more about my professional experience and qualifications, 
                please refer to my resume, which can be found <a href="pdf/cv_kilian_fatras.pdf" class="">here</a>. </p>
              
              <p>Thank you for visiting my website and feel free to contact me with any questions or inquiries.</p>
         
              
            </div>

            <div class="row">
            <hr class="bigHr">
            <center><h3>News !</h3></center>
            <hr>

                <ol style="list-style-type:circle">
                        <li> New pre-print: <a href="https://arxiv.org/abs/2310.02391"> Flow Matching for protein backbone generation</a>!</li>
                        <li> New pre-print: <a href="https://arxiv.org/abs/2309.09968"> Flow Matching with XGBoost for generating tabular data generation</a>!</li>
                        <li>I succesfully defended my PhD thesis "Optimal Transport and Deep Learning: Learning from one another" ! You can watch the defense <a href="https://www.youtube.com/watch?v=paqpidEnnHw">here</a>.</li>
                    </ol>
          </div>

            <div class="row"><a name="research_interests"></a>
            <hr class="bigHr">
            <center><h3>Research interests</h3></center>
            <hr>
                <p>
                My work focuses on <b>AI for Science</b>, <b>generative modeling</b>, <b>distribution shifts (domain adaptation, out-of-distribution samples, ...)</b> and <b>optimal transport</b>. 
                Recently, I developped a strong interest for biological applications with a focus on protein generation.
                </p>
          </div>
          <div class="row">
          <hr class="bigHr">
          <center><h3><a name="Talks"></a>Talks</h3></center>
          <hr>
          27/06/23 - <a href="http://gene.com"> Genentech </a>: Optimal transport to learn robust representations and trajectory inferences<br>
          11/04/23 - <a href="https://ai.facebook.com"> Montréal FAIR </a>: Evaluation of deep partial Domain Adaptation methods<br>
          28/02/23 - <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-montreal/"> Montréal Microsoft AI lab seminar</a>: Evaluation of deep partial Domain Adaptation methods<br>
          08/11/22 - <a href="http://dev3.noahlab.com.hk/"> Huawei (Noah's Ark Lab) seminar</a>: Evaluation of deep partial Domain Adaptation methods<br>
          04/12/22 - <a a href="https://winter22.cms.math.ca"> Canadian Mathematical Society winter meeting 2022</a>: Minibatch Optimal Transport distances in Deep Learning<br>
          25/11/22 - <a href="https://www.litislab.fr/"> LITIS seminar</a>: Evaluation of deep partial Domain Adaptation methods<br>
          07/04/22 - <a a href="https://cerc-datascience.polymtl.ca/coffee/"> DS4DM Coffee Talks Polytechnique Montréal</a>: Unbalanced minibatch Optimal Transport<br>
          01/09/21 - <a a href="https://portail.polytechnique.edu/cmap/fr"> CMAP Ecole Polytechnique</a>: Unbalanced minibatch Optimal Transport<br>
          28/04/21 - <a a href="https://mtl-mlopt.github.io/"> Montréal Machine Learning and Optimization (MTL MLOpt)</a>: Unbalanced minibatch Optimal Transport; applications to Domain Adaptation<br>
          09/07/19 - <a a href="http://gdr-isis.fr/index.php?page=reunion&idreunion=395">GDR-ISIS</a>: Transport optimal en apprentissage statistique et traitement du signal
        </div>
          <div class="row"><a name="papers"></a>
          <hr class="bigHr">
          <center><h3>Papers</h3></center>
          <hr>


          <p> <i><strong> SE(3)-Stochastic Flow Matching for Protein Backbone Generation
          </strong></i><br>
                     <i>Avishek Joey Bose, Tara Akhound-Sadegh, <b>Kilian Fatras</b>, Guillaume Huguet, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong
                    </i><br>
                      Preprint, 2023<br>
                     <i>Keywords: Flow matching, protein backbone design, SE(3) manifold, Equilibrium conformation generation</i> <br>  
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2310.02391">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/DreamFold/FoldFlow">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-17">Bibtex</a>
           <pre class="bibtex-17 collapse">
@misc{bose2023se3stochastic,
  title={SE(3)-Stochastic Flow Matching for Protein Backbone Generation}, 
  author={Avishek Joey Bose and Tara Akhound-Sadegh and Kilian Fatras and Guillaume Huguet and Jarrid Rector-Brooks and Cheng-Hao Liu and Andrei Cristian Nica and Maksym Korablyov and Michael Bronstein and Alexander Tong},
  year={2023},
  eprint={2310.02391},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
            </pre>
            </div>
            </br>


          <p> <i><strong> Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees
          </strong></i><br>
                     <i>Alexia Jolicoeur-Martineau, <b>Kilian Fatras</b>, Tal Kachman
                    </i><br>
                      Preprint, 2023<br>
                     <i>Keywords: Flow matching, diffusion models, XGBoost, tabular data</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2309.09968">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/SamsungSAILMontreal/ForestDiffusion">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-16">Bibtex</a>
           <pre class="bibtex-16 collapse">
@misc{jolicoeurmartineau2023generating,
  title={Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees}, 
  author={Alexia Jolicoeur-Martineau and Kilian Fatras and Tal Kachman},
  year={2023},
  eprint={2309.09968},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
            </pre>
            </div>
            </br>

          <p> <i><strong> No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths
          </strong></i><br>
                     <i>Charles Guille-Escuret, Hiroki Naganuma, <b>Kilian Fatras</b>, Ioannis Mitliagkas
                    </i><br>
                      Preprint, 2023<br>
                     <i>Keywords: Optimization, Neural network landscape</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2306.11922">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/Hiroki11x/LossLandscapeGeometry">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-15">Bibtex</a>
           <pre class="bibtex-15 collapse">
@misc{guilleescuret2023wrong,
  title={No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths}, 
  author={Charles Guille-Escuret and Hiroki Naganuma and Kilian Fatras and Ioannis Mitliagkas},
  year={2023},
  eprint={2306.11922},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
            </pre>
            </div>
            </br>


          <p> <i><strong> Simulation-free Schrödinger bridges via score and flow matching
          </strong></i><br>
                     <i>Alexander Tong*, Nikolay Malkin*, <b>Kilian Fatras*</b>, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, Yoshua Bengio
                    </i><br>
                    * Equal contribution<br>
                      <a href="https://frontiers4lcd.github.io/">ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</a>, 2023<br>
                     <i>Keywords: generative models, normalizing flows, optimal transport, single-cell dynamics</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2307.03672">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/atong01/conditional-flow-matching">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-14">Bibtex</a>
           <pre class="bibtex-14 collapse">
            @misc{tong2023simulationfree,
              title={Simulation-free Schr\"odinger bridges via score and flow matching}, 
              author={Alexander Tong and Nikolay Malkin and Kilian Fatras and Lazar Atanackovic and Yanlei Zhang and Guillaume Huguet and Guy Wolf and Yoshua Bengio},
              year={2023},
              eprint={2307.03672},
              archivePrefix={arXiv},
              primaryClass={cs.LG}
        }
            </pre>
            </div>
            </br>

          <p> <i><strong> Unbalanced Optimal Transport meets Sliced-Wasserstein
          </strong></i><br>
                     <i> Thibault Séjourné, Clément Bonet, <b>Kilian Fatras</b>, Kimia Nadjahi and Nicolas Courty
                    </i><br>
                    <a href="https://frontiers4lcd.github.io/">ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</a>, 2023<br>
                     
                     <i>Keywords: optimal transport, climate change, Wasserstein barycenter</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="http://arxiv.org/abs/2306.07176">ArXiv</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-13">Bibtex</a>
           <pre class="bibtex-13 collapse">
@misc{séjourné2023unbalanced,
  title={Unbalanced Optimal Transport meets Sliced-Wasserstein}, 
  author={Thibault Séjourné and Clément Bonet and Kilian Fatras and Kimia Nadjahi and Nicolas Courty},
  year={2023},
  eprint={2306.07176},
  archivePrefix={arXiv},
  primaryClass={cs.LG}}
           </pre>
               </div>
               </br>

          <p> <i><strong> PopulAtion Parameter Averaging (PAPA)
          </strong></i><br>
                     <i>Alexia Jolicoeur-Martineau, Emy Gervais, <b>Kilian Fatras</b>, Yan Zhang and Simon Lacoste-Julien
                    </i><br>
                     Preprint, 2023<br>
                     <i>Keywords: merging models, computer vision, remote sensing</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2304.03094">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/SamsungSAILMontreal/PAPA">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-12">Bibtex</a>
           <pre class="bibtex-12 collapse">
@misc{jolicoeurmartineau2023population,
  title={PopulAtion Parameter Averaging (PAPA)}, 
  author={Alexia Jolicoeur-Martineau and Emy Gervais and Kilian Fatras and Yan Zhang and Simon Lacoste-Julien},
  year={2023},
  eprint={2304.03094},
  archivePrefix={arXiv},
  primaryClass={cs.LG}}
           </pre>
               </div>
               </br>
          
          <p> <i><strong> Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport
          </strong></i><br>
                     <i>Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, <b>Kilian Fatras</b>, Guy Wolf, Yoshua Bengio
                    </i><br>
                      <a href="https://frontiers4lcd.github.io/">ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</a>, 2023<br>
                     <i>Keywords: generative models, normalizing flows, optimal transport, single-cell dynamics</i> <br>
         
         
                    <div class="btn-group-xs">
                       <a a class="btn btn-default" href="https://arxiv.org/abs/2302.00482">ArXiv</a>
                       <a a class="btn btn-default" href="https://github.com/atong01/conditional-flow-matching">Code</a>
                       <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-11">Bibtex</a>
           <pre class="bibtex-11 collapse">
@misc{https://doi.org/10.48550/arxiv.2302.00482,
doi = {10.48550/ARXIV.2302.00482},
url = {https://arxiv.org/abs/2302.00482},
author = {Tong, Alexander and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Fatras, Kilian and Wolf, Guy and Bengio, Yoshua},
keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
title = {Conditional Flow Matching: Simulation-Free Dynamic Optimal Transport},
publisher = {arXiv},
year = {2023},
copyright = {arXiv.org perpetual, non-exclusive license}
}
           </pre>
               </div>
               </br>
          


          <p> <i><strong> A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods
          </strong></i><br>
            <i>Tiago Salvador*, <b>Kilian Fatras</b>*, Ioannis Mitliagkas, Adam Oberman</i><br>
            * Equal contribution<br>
            Transactions on Machine Learning Research (<a href="https://jmlr.org/tmlr/">TMLR</a>), 2023<br>
            <i>Keywords: Partial domain adaptation, reproducibility, benchmark</i> <br>


           <div class="btn-group-xs">
              <a a class="btn btn-default" href="https://arxiv.org/abs/2210.01210">ArXiv</a>
              <a a class="btn btn-default" href="https://github.com/oberman-lab/BenchmarkPDA">Code</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-10">Bibtex</a>
          <pre class="bibtex-10 collapse">
            @article{
              salvador2023a,
              title={A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods},
              author={Tiago Salvador and Kilian FATRAS and Ioannis Mitliagkas and Adam M Oberman},
              journal={Transactions on Machine Learning Research},
              issn={2835-8856},
              year={2023},
              url={https://openreview.net/forum?id=XcVzIBXeRn},
              note={}
              }

          </pre>
          </div>
          </br>

          <p> <i><strong> On making optimal transport robust to all outliers
 </strong></i><br>
            <i><b>Kilian Fatras</b></i><br>
            Preprint, 2022<br>
            <i>Keywords: Optimal Transport, Noisy labels, Generative models</i> <br>


           <div class="btn-group-xs">
              <a a class="btn btn-default" href="http://arxiv.org/abs/2206.11988">ArXiv</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-9">Bibtex</a>
  <pre class="bibtex-9 collapse">
@misc{https://doi.org/10.48550/arxiv.2206.11988,
  doi = {10.48550/ARXIV.2206.11988},

  url = {https://arxiv.org/abs/2206.11988},

  author = {Fatras, Kilian},

  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Probability (math.PR), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},

  title = {On making optimal transport robust to all outliers},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}
  </pre>
      </div>
      </br>

          <p> <i><strong> Optimal transport meets noisy label robust loss and MixUp for domain adaptation</strong></i><br>
            <i><b>Kilian Fatras</b>, Hiroki Naganuma, Ioannis Mitliagkas</i><br>
            Conference on Lifelong Learning Agents <a href="https://lifelong-ml.cc/">(CoLLAs)</a>, 2022<br>
            <i>Keywords: Optimal Transport, Noisy labels, MixUp, Domain Adaptation</i> <br>


           <div class="btn-group-xs">
             <a a class="btn btn-default" href="https://proceedings.mlr.press/v199/fatras22a.html">Paper</a>
              <a a class="btn btn-default" href="https://arxiv.org/abs/2206.11180">ArXiv</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-8">Bibtex</a>
  <pre class="bibtex-8 collapse">

    @InProceedings{fatras22aMixOT,
      title = 	 {Optimal Transport meets Noisy Label Robust Loss and MixUp Regularization for Domain Adaptation},
      author =       {Fatras, Kilian and Naganuma, Hiroki and Mitliagkas, Ioannis},
      booktitle = 	 {Proceedings of The 1st Conference on Lifelong Learning Agents},
      pages = 	 {966--981},
      year = 	 {2022},
      editor = 	 {Chandar, Sarath and Pascanu, Razvan and Precup, Doina},
      volume = 	 {199},
      series = 	 {Proceedings of Machine Learning Research},
      month = 	 {22--24 Aug},
      publisher =    {PMLR},
      pdf = 	 {https://proceedings.mlr.press/v199/fatras22a/fatras22a.pdf},
      url = 	 {https://proceedings.mlr.press/v199/fatras22a.html},
      abstract = 	 {It is common in computer vision to be confronted with domain shift: images which have the same class but different acquisition conditions. In domain adaptation (DA), one wants to classify unlabeled target images using source labeled images. Unfortunately, deep neural networks trained on a source training set perform poorly on target images which do not belong to the training domain. One strategy to improve these performances is to align the source and target image distributions in an embedded space using optimal transport (OT). To compute OT, most methods use the minibatch optimal transport approximation which causes negative transfer, i.e. aligning samples with different labels, and leads to overfitting. In this work, we mitigate negative alignment by explaining it as a noisy label assignment to target images. We then mitigate its effect by appropriate regularization. We propose to couple the MixUp regularization with a loss that is robust to noisy labels in order to improve domain adaptation performance. We show in an extensive ablation study that a combination of the two techniques is critical to achieve improved performance. Finally, we evaluate our method, called mixunbot, on several benchmarks and real-world DA problems.}
    }
  </pre>
      </div>
      </br>

          <p> <i><strong> POT: Python Optimal Transport </strong></i><br>
            <i> Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya,
              Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos,
              <b>Kilian Fatras</b>, Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud, Hicham Janati,
              Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy,
              Danica J. Sutherland, Alexander Tong and Titouan Vayer </i><br>
            Journal of Machine Learning Research <a href="https://www.jmlr.org/">(JMLR)</a> - Open Source Software, 2021<br>
            <div class="btn-group-xs">
              <a a class="btn btn-default" href="https://jmlr.org/papers/v22/20-451.html">Paper</a>
              <a a class="btn btn-default" href="https://pythonot.github.io/">Website</a>
              <a a class="btn btn-default" href="https://github.com/PythonOT/POT">Code</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-7">Bibtex</a>
<pre class="bibtex-7 collapse">
  @article{JMLR:v22:20-451,
    author  = {R\'emi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur\'elie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L\'eo Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
    title   = {POT: Python Optimal Transport},
    journal = {Journal of Machine Learning Research},
    year    = {2021},
    volume  = {22},
    number  = {78},
    pages   = {1-8},
    url     = {http://jmlr.org/papers/v22/20-451.html}
  }
</pre>
    </div>
    </br>

          <p> <i><strong> Unbalanced minibatch Optimal Transport; applications to Domain Adaptation</strong></i><br>
            <i><b>Kilian Fatras</b>, Thibault Séjourné, Nicolas Courty and Rémi Flamary</i><br>
            International Conference on Machine Learning <a href="https://icml.cc/Conferences/2021">(ICML)</a>, 2021<br>
            <i>Keywords: Unbalanced Optimal Transport, Minibatch, Concentration Bounds, (Partial) Domain Adaptation</i> <br>

           <div class="btn-group-xs">
              <a a class="btn btn-default" href="http://proceedings.mlr.press/v139/fatras21a.html">Paper</a>
              <a a class="btn btn-default" href="https://arxiv.org/abs/2103.03606">ArXiv</a>
              <a a class="btn btn-default" href="https://github.com/kilianFatras/JUMBOT">Code</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-6">Bibtex</a>
<pre class="bibtex-6 collapse">
@InProceedings{pmlr-v139-fatras21a,
title = 	 {Unbalanced minibatch Optimal Transport; applications to Domain Adaptation},
author =       {Fatras, Kilian and Sejourne, Thibault and Flamary, R{\'e}mi and Courty, Nicolas},
booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
pages = 	 {3186--3197},
year = 	 {2021},
editor = 	 {Meila, Marina and Zhang, Tong},
volume = 	 {139},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {18--24 Jul},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v139/fatras21a/fatras21a.pdf},
url = 	 {http://proceedings.mlr.press/v139/fatras21a.html},
abstract = 	 {Optimal transport distances have found many applications in machine learning for their capacity to compare non-parametric probability distributions. Yet their algorithmic complexity generally prevents their direct use on large scale datasets. Among the possible strategies to alleviate this issue, practitioners can rely on computing estimates of these distances over subsets of data, i.e. minibatches. While computationally appealing, we highlight in this paper some limits of this strategy, arguing it can lead to undesirable smoothing effects. As an alternative, we suggest that the same minibatch strategy coupled with unbalanced optimal transport can yield more robust behaviors. We discuss the associated theoretical properties, such as unbiased estimators, existence of gradients and concentration bounds. Our experimental study shows that in challenging problems associated to domain adaptation, the use of unbalanced optimal transport leads to significantly better results, competing with or surpassing recent baselines.}
}
</pre>
    </div>
    </br>

          <p> <i><strong> Minibatch Optimal Transport distances; analysis and applications</strong></i><br>
            <i><b>Kilian Fatras</b>, Younes Zine, Szymon Majewski, Rémi Flamary, Rémi Gribonval and Nicolas Courty</i><br>
            Preprint, 2021<br>
            <i>Keywords: Optimal Transport, Minibatch, Concentration Bounds, GANs, Sub-Gaussian data</i> <br>


           <div class="btn-group-xs">
              <a a class="btn btn-default" href="https://arxiv.org/abs/2101.01792">ArXiv</a>
              <a a class="btn btn-default" href="https://github.com/kilianFatras/unbiased_minibatch_sinkhorn_GAN">Code</a>
              <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-5">Bibtex</a>
<pre class="bibtex-5 collapse">
  @misc{fatras2021minibatch,
    title={Minibatch optimal transport distances; analysis and applications},
    author={Kilian Fatras and Younes Zine and Szymon Majewski and Rémi Flamary and Rémi Gribonval and Nicolas Courty},
    year={2021},
    eprint={2101.01792},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
</pre>
    </div>
    </br>
            <p> <i><strong> Generating natural adversarial Remote Sensing Images</strong></i><br>
            <i>Jean-Christophe Burnel, <b>Kilian Fatras</b>, Rémi Flamary and Nicolas Courty</i><br>
            IEEE Transactions on Geoscience and Remote Sensing <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"> (TGRS)</a>, 2021<br>
            <i>Keywords: Optimal Transport, GANs, Adversarial Examples, Remote Sensing</i> <br>

           <div class="btn-group-xs">
                <a a class="btn btn-default" href="https://ieeexplore.ieee.org/abstract/document/9542938">Paper</a>
                <a a class="btn btn-default" href="https://hal.archives-ouvertes.fr/hal-02558542/">ArXiv/HAL</a>
                <a a class="btn btn-default" href="https://github.com/PythonOT/ARWGAN">Code</a>
                <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-4">Bibtex</a>

    <pre class="bibtex-4 collapse">
@ARTICLE{burnel2021,
  author={Burnel, Jean-Christophe and Fatras, Kilian and Flamary, R{\'e}mi and Courty, Nicolas},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  title={Generating natural adversarial Remote Sensing Images},
  year={(to appear) 2021}}
</pre>
    </div>
            </br>

            <p> <i><strong> Learning with minibatch Wasserstein: asymptotic and gradient properties</strong></i><br>
            <i><b>Kilian Fatras</b>, Younes Zine, Rémi Flamary, Rémi Gribonval and Nicolas Courty</i><br>
            Proceedings of the 23nd International Conference on Artificial Intelligence and Statistics <a href="http://www.aistats.org/">(AISTATS)</a>, 2020<br>
            <i>Keywords: Optimal Transport, Minibatch, Concentration Bounds, Large Scale Color Transfer</i> <br>
           <div class="btn-group-xs">
               <a a class="btn btn-default" href="http://proceedings.mlr.press/v108/fatras20a.html">Paper</a>
               <a a class="btn btn-default" href="https://arxiv.org/abs/1910.04091">ArXiv</a>
               <a a class="btn btn-default" href="https://github.com/kilianFatras/minibatch_Wasserstein">Code</a>
               <a a class="btn btn-default" href="https://github.com/kilianFatras/minibatch_Wasserstein/blob/master/communication/mb_slides.pdf">Slides</a>
               <a a class="btn btn-default" href="https://github.com/kilianFatras/minibatch_Wasserstein/blob/master/communication/MBWasserstein_poster.pdf">Poster</a>
               <a a class="btn btn-default" href="https://medium.com/p/learning-with-minibatch-wasserstein-d87dcf52efb5?source=email-d0d7857135bb--writer.postDistributed&sk=4c30efd3442780edf7ca140080557476">Blog</a>
                <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-3">Bibtex</a>
<pre class="bibtex-3 collapse">
@InProceedings{pmlr-v108-fatras20a,
  title = 	 {Learning with minibatch Wasserstein  : asymptotic and gradient properties},
  author = 	 {Fatras, Kilian and Zine, Younes and Flamary, R\'emi and Gribonval, Remi and Courty, Nicolas},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2131--2141},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/fatras20a/fatras20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/fatras20a.html},
  abstract = 	 {Optimal transport distances are powerful tools to compare probability distributions and have found many applications in machine learning. Yet their algorithmic complexity prevents their direct use on large scale datasets. To overcome this challenge, practitioners compute these distances on minibatches i.e., they average the outcome of several smaller optimal transport problems. We propose in this paper an analysis of this practice, which effects are not well understood so far. We notably argue that it is equivalent to an implicit regularization of the original problem, with appealing properties such as unbiased estimators, gradients and a concentration bound around the expectation, but also with defects such as loss of distance property. Along with this theoretical analysis, we also conduct empirical experiments on gradient flows, GANs or color transfer that highlight the practical interest of this strategy.}
}
</pre>
    </div>
    </br>
      <p> <i><strong> Wasserstein Adversarial Regularization (WAR) on label noise</strong></i><br>
            <i><b>Kilian Fatras</b>*, Bharath Damodaran*, Sylvain Lobry, Rémi Flamary, Devis Tuia and Nicolas Courty</i><br>
            <i>* Equal contribution</i><br>
            IEEE Transactions on Pattern Analysis and Machine Intelligence
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"> (TPAMI)</a>, 2021<br>
            <i>Keywords: Optimal Transport, Adversarial Training, label noise, Remote Sensing</i> <br>

           <div class="btn-group-xs">
                <a a class="btn btn-default" href="https://ieeexplore.ieee.org/document/9477020">Paper</a>
                <a a class="btn btn-default" href="https://arxiv.org/abs/1904.03936">ArXiv</a>
                <a a class="btn btn-default" href="https://github.com/bbdamodaran/WAR">Code</a>
                <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-war">Bibtex</a>

    <pre class="bibtex-war collapse">
@ARTICLE{Fatras2021WAR,
author={Fatras, Kilian and Damodaran, Bharath Bhushan and Lobry, Sylvain and Flamary, Remi and Tuia, Devis and Courty, Nicolas},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Wasserstein Adversarial Regularization for learning with label noise},
year={2021},
doi={10.1109/TPAMI.2021.3094662}}</pre>
    </div>
            </br>

	   <p> <i><strong> Proximal Splitting Meets Variance Reduction</strong></i><br>
            <i>Fabian Pedregosa, <b>Kilian Fatras</b> and Mattia Casotto.</i><br>
           Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics <a href="http://www.aistats.org/">(AISTATS)</a>, 2019<br>
           <i>Keywords: Proximal Splitting, Variance Reduction, Sparse Update</i> <br>


              <div class="btn-group-xs">
                <a a class="btn btn-default" href="http://proceedings.mlr.press/v89/pedregosa19a.html">Paper</a>
                <a a class="btn btn-default" href="https://arxiv.org/abs/1806.07294">ArXiv</a>
                <a a class="btn btn-default" href="https://github.com/openopt/copt/blob/master/copt/randomized.py">Code</a>
                <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-1">Bibtex</a>

    <pre class="bibtex-1 collapse">
@InProceedings{pmlr-v89-pedregosa19a,
title = 	 {Proximal Splitting Meets Variance Reduction},
author =       {Pedregosa, Fabian and Fatras, Kilian and Casotto, Mattia},
booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
pages = 	 {1--10},
year = 	 {2019},
editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
volume = 	 {89},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {16--18 Apr},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v89/pedregosa19a/pedregosa19a.pdf},
url = 	 {http://proceedings.mlr.press/v89/pedregosa19a.html},
abstract = 	 {Despite the raise to fame of stochastic variance reduced methods like SAGA and ProxSVRG, their use in non-smooth optimization is still limited to a few simple cases. Existing methods require to compute the proximal operator of the non-smooth term at each iteration, which, for complex penalties like the total variation, overlapping group lasso or trend filtering, is an iterative process that becomes unfeasible for moderately large problems. In this work we propose and analyze VRTOS, a variance-reduced method to solve problems with an arbitrary number of non-smooth terms. Like other variance reduced methods, it only requires to evaluate one gradient per iteration and converges with a constant step size, and so is ideally suited for large scale applications. Unlike existing variance reduced methods, it admits multiple non-smooth terms whose proximal operator only needs to be evaluated once per iteration. We provide a convergence rate analysis for the proposed methods that achieves the same asymptotic rate as their full gradient variants and illustrate its computational advantage on 4 different large scale datasets.}
}</pre>
    </div>

            <br>
            </div>

              <div class="row">
                <hr class="bigHr">
                <center><h3><a name="Workshops"></a>Workshops</h3></center>
                <hr>
                <p> <i><strong> A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods
                </strong></i><br>
                  <i>Tiago Salvador*, <b>Kilian Fatras</b>*, Ioannis Mitliagkas, Adam Oberman</i><br>
                  * Equal contributions<br>
                  Workshop on Distribution Shifts, 36th Conference on Neural Processing Systems (<a href="https://sites.google.com/view/distshift2022">NeurIPS 2022</a>)<br>
                  <i>Keywords: Partial domain adaptation, reproducibility, benchmark</i> <br>


                 <div class="btn-group-xs">
                    <a a class="btn btn-default" href="https://arxiv.org/abs/2210.01210">ArXiv</a>
                    <a a class="btn btn-default" href="https://github.com/oberman-lab/BenchmarkPDA">Code</a>
                    <a a href="#/" class="btn btn-default" data-toggle="collapse" data-target=".bibtex-w-1">Bibtex</a>
                <pre class="bibtex-w-1 collapse">
                  @misc{https://doi.org/10.48550/arxiv.2210.01210,
                    doi = {10.48550/ARXIV.2210.01210},

                    url = {https://arxiv.org/abs/2210.01210},

                    author = {Salvador, Tiago and Fatras, Kilian and Mitliagkas, Ioannis and Oberman, Adam},

                    keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

                    title = {A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods},

                    publisher = {arXiv},

                    year = {2022},

                    copyright = {arXiv.org perpetual, non-exclusive license}
                  }

                </pre>
                </div>
                </br>              </div>

              <div class="row">
                  <hr class="bigHr">
                  <center><h3><a name="Lectures"></a>Lectures</h3></center>
                  <hr>
                  04/22 - <a a href="pdf/intro_OT.pdf"> Introduction to Optimal Transport </a> - Université de Montréal and McGill University<br>
              </div>
              <div class="row">
                <hr class="bigHr">
                <center><h3><a name="Projects"></a>Projects and Volunteering</h3></center>
                <hr>
                        <p>Here is a list of my volunteering activities and the different projects I contribute to:</p>
                    <ol style="list-style-type:circle">
                      <li> 11/18/21 - <b>Organization</b> of GDR ISIS & MIA (in person) workshop on <a href="https://www.gdr-isis.fr/index.php/reunion/461/"> optimal transport and statistical learning </a> !                      </li>
                        <li><b>Python for Optimal Transport <a href="https://pot.readthedocs.io/en/stable/">(POT)</a> </b> is an open source library for optimal transport in Python.</li>

                        <li><b>Reviewer</b> for <b><a href="http://www.jmlr.org/">JMLR</a>, <a href="https://icml.cc/Conferences/2021">ICML</a>, <a href="https://2021.ecmlpkdd.org/">ECML</a>, <a href="https://www.springer.com/journal/10957">JOTA</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36">IEEE TGRS</a>, <a href="https://iclr.cc/">ICLR</a>, <a href="https://aistats.org/aistats2022/">AISTATS</a>, <a href="https://neurips.cc/">NeurIPS</a></b>.</li>
                        <li><b>Best reviewer award</b> <b><a href="https://iclr.cc/">ICLR 2022</a></b>.</li>
                    </ol>
                </div>
              <div class="row">
                <hr class="bigHr">
                <center><h3><a name="contact"></a>Contacts</h3></center>
                <hr>
                <address>
                    <div>
                    <p>I use several networks, do not hesitate to reach me out !
                    </p>
                    </div>

                </address>
              </div>
                <div id="social-link">
                  <a href="https://www.linkedin.com/in/kilianfatras" style="text-decoration:none;"><span style="font: 80% Arial,sans-serif; color:#0783B6;"><img src="./icons/linkedin.png" height="20" width="20" alt="See LinkedIn profile" style="vertical-align:middle;" border="0" onmouseover="this.src='./icons/mouseoverlinkedin.png';" onmouseout="this.src='./icons/linkedin.png';" ></span></a>
                  <a href="https://github.com/kilianFatras" style="text-decoration:none;"><span style="font: 80% Arial,sans-serif; color:#0783B6;"><img src="./icons/Octocat.png" height="20" width="20" alt="See GitHub profile" style="vertical-align:middle;" border="0" onmouseover="this.src='./icons/GitHub-Mark-32px.png';" onmouseout="this.src='./icons/Octocat.png';" ></span></a>
                  <a href="https://twitter.com/FatrasKilian" style="text-decoration:none;"><span style="font: 80% Arial,sans-serif; color:#0783B6;"><img src="./icons/Twitter_Logo_Blue.png" height="20" width="20" alt="Send an email" style="vertical-align:middle;" border="0" onmouseover="this.src='./icons/Twitter_Logo_WhiteOnImage.png';" onmouseout="this.src='./icons/Twitter_Logo_Blue.png';" ></span></a>
                  <a href="mailto:kilian dot fatras.pro at gmail dot com" style="text-decoration:none;"><span style="font: 80% Arial,sans-serif; color:#0783B6;"><img src="./icons/mail.png" height="20" width="20" alt="Send an email" style="vertical-align:middle;" border="0" onmouseover="this.src='./icons/mouseovermail.png';" onmouseout="this.src='./icons/mail.png';" ></span></a>
                </div>
              </div>
            </div>
          <footer>&copy; 2018</footer>
        <!-- Bootstrap core JavaScript
        ================================================== -->
        <!-- Placed at the end of the document so the pages load faster -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
        <script type='text/javascript' src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
        <script src="offcanvas.js"></script>
        <script src="code.js"></script>
        </div>
      </body>
    </html>
